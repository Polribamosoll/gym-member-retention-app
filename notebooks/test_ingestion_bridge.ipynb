{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Ready to demonstrate data ingestion and normalization.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from io import StringIO, BytesIO\n",
        "\n",
        "# Add the parent directory to the system path to allow importing from dat-ingestion_bridge\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "# Explicitly import from the correct path\n",
        "from src.ingestion_utils import process_gym_data, IngestionMetadata\n",
        "\n",
        "# Create a temporary directory for test files\n",
        "if not os.path.exists(\"temp_data\"):\n",
        "    os.makedirs(\"temp_data\")\n",
        "\n",
        "print(\"Setup complete. Ready to demonstrate data ingestion and normalization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Possible Data Formats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scenario 1: CSV with inconsistent delimiters and encoding issues\n",
        "csv_content_bad_delimiter = \"\"\"\n",
        "id;name;age;start_date\n",
        "1;Alice;30;2022-01-15\n",
        "2,Bob,24,16/03/2021\n",
        "3;Charlie;35;2020-11-01\n",
        "4,David,29,05-07-2023\n",
        "\"\"\"\n",
        "csv_path_bad_delimiter = \"temp_data/bad_delimiter_data.csv\"\n",
        "\n",
        "with open(csv_path_bad_delimiter, \"w\", encoding=\"latin1\") as f:\n",
        "    f.write(csv_content_bad_delimiter)\n",
        "\n",
        "\n",
        "# Scenario 2\n",
        "excel_path_junk_empty = \"temp_data/junk_empty_data.xlsx\"\n",
        "data_excel = {\n",
        "    'MemberID': ['M001', 'M002', 'M003', 'M004', 'M005', None, 'M007'],\n",
        "    'Name': ['Eve', 'Frank', 'Grace', None, 'Heidi', 'Ivy', 'Jack'],\n",
        "    'MembershipType': ['Gold', 'Silver', 'Gold', 'Bronze', 'Silver', 'Gold', 'Bronze'],\n",
        "    'LastVisit': ['2023-01-10', '15/02/2023', '2023-03-20', 'invalid-date', '2023-05-01', None, '2023-06-11'],\n",
        "    'Fee': [100.50, 75, 100.50, 50, 75, 100.50, 50],\n",
        "    'IsActive': ['YES', 'No', '1', '0', 'true', 'FALSE', None],\n",
        "    'Junk1': [None, None, None, None, None, None, None], # Empty column\n",
        "    'Junk2': ['garbage', 'garbage', 'garbage', 'garbage', 'garbage', 'garbage', 'garbage']\n",
        "}\n",
        "df_excel_raw = pd.DataFrame(data_excel)\n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\", message=\"The behavior of DataFrame concatenation with empty or all-NA entries is deprecated\", category=FutureWarning)\n",
        "    df_excel_raw.loc[len(df_excel_raw)] = [None] * len(df_excel_raw.columns)\n",
        "\n",
        "with pd.ExcelWriter(excel_path_junk_empty, engine='openpyxl') as writer:\n",
        "    df_excel_raw.to_excel(writer, index=False, sheet_name='Sheet1')\n",
        "\n",
        "\n",
        "# Scenario 3: TSV with long format and missing values\n",
        "tsv_content_long_missing = \"\"\"\n",
        "member_id\\tattribute\\tvalue\n",
        "101\\tgender\\tMale\n",
        "101\\tage\\t28\n",
        "102\\tgender\\tFemale\n",
        "102\\tage\\t34\n",
        "103\\tgender\\tNone\n",
        "103\\tage\\t30\n",
        "104\\tgender\\tMale\n",
        "104\\tage\\t\n",
        "105\\tgender\\tFemale\n",
        "105\\tage\\t22\n",
        "\"\"\"\n",
        "tsv_path_long_missing = \"temp_data/long_missing_data.tsv\"\n",
        "with open(tsv_path_long_missing, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(tsv_content_long_missing)\n",
        "\n",
        "\n",
        "# Scenario 4: CSV with mixed types, varying date formats, and partial corruption\n",
        "csv_content_mixed_corrupt = \"\"\"\n",
        "OrderID,Customer,OrderDate,Amount,Status,DeliveryDate\n",
        "1001,John Doe,2023-01-01,150.75,Completed,2023-01-05\n",
        "1002,Jane Smith,02/01/2023,abc,Pending,06/01/2023\n",
        "1003,Peter Jones,2023-Mar-03,200.00,Completed,2023-03-07\n",
        "1004,Alice Brown,04.04.2023,75.20,Cancelled,invalid-date\n",
        "1005,Bob White,2023/05/05,120,Completed,2023-May-09\n",
        "1006,Charlie Green,06-Jun-2023,300.00,Pending,10-Jun-2023\n",
        "1007,Diana Prince,07/Jul/2023,100,Completed,11/07/2023\n",
        "1008,Eve Black,invalid-date,50.00,Pending,2023-08-15\n",
        "\"\"\"\n",
        "csv_path_mixed_corrupt = \"temp_data/mixed_corrupt_data.csv\"\n",
        "with open(csv_path_mixed_corrupt, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(csv_content_mixed_corrupt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SCENARIO 1: Mixed Delimiters CSV ===\n",
            "\n",
            "üìä DataFrame Shape: 4 rows √ó 4 columns\n",
            "\n",
            "üè∑Ô∏è  Columns Identified: ['id', 'name', 'age', 'start_date']\n",
            "\n",
            "üéØ Column Roles: {'id': 'Numeric metric', 'name': 'Categorical dimension', 'age': 'Numeric metric', 'start_date': 'Date/time'}\n",
            "\n",
            "üìÅ File Type: csv\n",
            "\n",
            "üìà Orientation: wide\n",
            "\n",
            "--- Sample Data ---\n",
            "   id     name  age  start_date\n",
            "0   1    Alice   30  2022-01-15\n",
            "1   2      Bob   24  2021-03-16\n",
            "2   3  Charlie   35  2020-11-01\n",
            "3   4    David   29  2023-07-05\n",
            "\n",
            "--- Data Types ---\n",
            "  id: object ‚Üí Int64\n",
            "  name: object ‚Üí string\n",
            "  age: object ‚Üí Int64\n",
            "  start_date: object ‚Üí string\n",
            "‚ö†Ô∏è  Warnings (6):\n",
            "   ‚Ä¢ Delimiter ';' with encoding 'utf-8' resulted in inconsistent column counts across rows. Trying other options.\n",
            "   ‚Ä¢ Failed to load with delimiter ',' and encoding 'utf-8': Expected 1 fields in line 4, saw 4\n",
            "   ‚Ä¢ Attempted to load with delimiter '\t' and encoding 'utf-8', but resulted in a single column. Trying other options.\n",
            "   ‚Ä¢ Attempted to load with delimiter '|' and encoding 'utf-8', but resulted in a single column. Trying other options.\n",
            "   ‚Ä¢ File appears to have inconsistent delimiters. Attempting fallback parsing.\n",
            "   ‚Ä¢ Successfully parsed file with mixed delimiters using fallback method.\n"
          ]
        }
      ],
      "source": [
        "# Scenario 1: CSV with inconsistent delimiters and encoding issues\n",
        "\n",
        "df_processed, metadata = process_gym_data(csv_path_bad_delimiter)\n",
        "\n",
        "print(\"\\n=== SCENARIO 1: Mixed Delimiters CSV ===\")\n",
        "print(f\"\\nüìä DataFrame Shape: {df_processed.shape[0]} rows √ó {df_processed.shape[1]} columns\")\n",
        "print(f\"\\nüè∑Ô∏è  Columns Identified: {list(df_processed.columns)}\")\n",
        "print(f\"\\nüéØ Column Roles: {metadata.column_roles}\")\n",
        "print(f\"\\nüìÅ File Type: {metadata.file_type}\")\n",
        "print(f\"\\nüìà Orientation: {metadata.orientation}\")\n",
        "\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "print(df_processed.head())\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "for col, dtype in metadata.inferred_dtypes.items():\n",
        "    original = metadata.original_dtypes.get(col, 'unknown')\n",
        "    print(f\"  {col}: {original} ‚Üí {dtype}\")\n",
        "\n",
        "if metadata.warnings:\n",
        "    print(f\"‚ö†Ô∏è  Warnings ({len(metadata.warnings)}):\")\n",
        "    for warning in metadata.warnings:\n",
        "        print(f\"   ‚Ä¢ {warning}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No warnings generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SCENARIO 2: Excel with Junk Data ===\n",
            "\n",
            "üìä DataFrame Shape: 7 rows √ó 7 columns\n",
            "\n",
            "üè∑Ô∏è  Columns Identified: ['MemberID', 'Name', 'MembershipType', 'LastVisit', 'Fee', 'IsActive', 'Junk2']\n",
            "\n",
            "üéØ Column Roles: {'MemberID': 'Categorical dimension', 'Name': 'Categorical dimension', 'MembershipType': 'Categorical dimension', 'LastVisit': 'Other', 'Fee': 'Numeric metric', 'IsActive': 'Numeric metric', 'Junk2': 'Categorical dimension'}\n",
            "\n",
            "üìÅ File Type: xlsx\n",
            "\n",
            "üìà Orientation: long\n",
            "\n",
            "--- Sample Data ---\n",
            "  MemberID   Name MembershipType   LastVisit    Fee  IsActive    Junk2\n",
            "0     M001    Eve           Gold  2023-01-10  100.5      True  garbage\n",
            "1     M002  Frank         Silver  2023-02-15   75.0     False  garbage\n",
            "2     M003  Grace           Gold  2023-03-20  100.5      True  garbage\n",
            "3     M004   <NA>         Bronze         NaN   50.0     False  garbage\n",
            "4     M005  Heidi         Silver  2023-05-01   75.0      True  garbage\n",
            "\n",
            "--- Data Types ---\n",
            "  MemberID: object ‚Üí string\n",
            "  Name: object ‚Üí string\n",
            "  MembershipType: object ‚Üí category\n",
            "  LastVisit: object ‚Üí string\n",
            "  Fee: float64 ‚Üí Float64\n",
            "  IsActive: object ‚Üí boolean\n",
            "  Junk2: object ‚Üí string\n",
            "‚ö†Ô∏è  Warnings (3):\n",
            "   ‚Ä¢ Dropped 1 entirely empty columns.\n",
            "   ‚Ä¢ Column 'LastVisit' was coerced to datetime, but 28.57% of values were unparseable. Standardized to YYYY-MM-DD format.\n",
            "   ‚Ä¢ Column 'IsActive' was coerced to boolean, but 14.29% of values were unparseable.\n"
          ]
        }
      ],
      "source": [
        "# Scenario 2: Excel file with junk columns, empty rows, and mixed data types\n",
        "\n",
        "df_processed_excel, metadata_excel = process_gym_data(excel_path_junk_empty)\n",
        "\n",
        "print(\"\\n=== SCENARIO 2: Excel with Junk Data ===\")\n",
        "print(f\"\\nüìä DataFrame Shape: {df_processed_excel.shape[0]} rows √ó {df_processed_excel.shape[1]} columns\")\n",
        "print(f\"\\nüè∑Ô∏è  Columns Identified: {list(df_processed_excel.columns)}\")\n",
        "print(f\"\\nüéØ Column Roles: {metadata_excel.column_roles}\")\n",
        "print(f\"\\nüìÅ File Type: {metadata_excel.file_type}\")\n",
        "print(f\"\\nüìà Orientation: {metadata_excel.orientation}\")\n",
        "\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "print(df_processed_excel.head())\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "for col, dtype in metadata_excel.inferred_dtypes.items():\n",
        "    original = metadata_excel.original_dtypes.get(col, 'unknown')\n",
        "    print(f\"  {col}: {original} ‚Üí {dtype}\")\n",
        "\n",
        "if metadata_excel.warnings:\n",
        "    print(f\"‚ö†Ô∏è  Warnings ({len(metadata_excel.warnings)}):\")\n",
        "    for warning in metadata_excel.warnings:\n",
        "        print(f\"   ‚Ä¢ {warning}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No warnings generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SCENARIO 3: Long Format TSV ===\n",
            "\n",
            "üìä DataFrame Shape: 10 rows √ó 3 columns\n",
            "\n",
            "üè∑Ô∏è  Columns Identified: ['member_id', 'attribute', 'value']\n",
            "\n",
            "üéØ Column Roles: {'member_id': 'Numeric metric', 'attribute': 'Categorical dimension', 'value': 'Categorical dimension'}\n",
            "\n",
            "üìÅ File Type: tsv\n",
            "\n",
            "üìà Orientation: long\n",
            "\n",
            "--- Sample Data ---\n",
            "   member_id attribute   value\n",
            "0        101    gender    Male\n",
            "1        101       age      28\n",
            "2        102    gender  Female\n",
            "3        102       age      34\n",
            "4        103    gender    <NA>\n",
            "\n",
            "--- Data Types ---\n",
            "  member_id: int64 ‚Üí Int64\n",
            "  attribute: object ‚Üí category\n",
            "  value: object ‚Üí string\n",
            "\n",
            "‚úÖ No warnings generated\n"
          ]
        }
      ],
      "source": [
        "# Scenario 3: TSV with long format and missing values\n",
        "\n",
        "df_processed_tsv, metadata_tsv = process_gym_data(tsv_path_long_missing)\n",
        "\n",
        "print(\"\\n=== SCENARIO 3: Long Format TSV ===\")\n",
        "print(f\"\\nüìä DataFrame Shape: {df_processed_tsv.shape[0]} rows √ó {df_processed_tsv.shape[1]} columns\")\n",
        "print(f\"\\nüè∑Ô∏è  Columns Identified: {list(df_processed_tsv.columns)}\")\n",
        "print(f\"\\nüéØ Column Roles: {metadata_tsv.column_roles}\")\n",
        "print(f\"\\nüìÅ File Type: {metadata_tsv.file_type}\")\n",
        "print(f\"\\nüìà Orientation: {metadata_tsv.orientation}\")\n",
        "\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "print(df_processed_tsv.head())\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "for col, dtype in metadata_tsv.inferred_dtypes.items():\n",
        "    original = metadata_tsv.original_dtypes.get(col, 'unknown')\n",
        "    print(f\"  {col}: {original} ‚Üí {dtype}\")\n",
        "\n",
        "if metadata_tsv.warnings:\n",
        "    print(f\"‚ö†Ô∏è  Warnings ({len(metadata_tsv.warnings)}):\")\n",
        "    for warning in metadata_tsv.warnings:\n",
        "        print(f\"   ‚Ä¢ {warning}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No warnings generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scenario 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== SCENARIO 4: Mixed Types & Corruption ===\n",
            "\n",
            "üìä DataFrame Shape: 8 rows √ó 6 columns\n",
            "\n",
            "üè∑Ô∏è  Columns Identified: ['OrderID', 'Customer', 'OrderDate', 'Amount', 'Status', 'DeliveryDate']\n",
            "\n",
            "üéØ Column Roles: {'OrderID': 'Numeric metric', 'Customer': 'Categorical dimension', 'OrderDate': 'Other', 'Amount': 'Numeric metric', 'Status': 'Categorical dimension', 'DeliveryDate': 'Other'}\n",
            "\n",
            "üìÅ File Type: csv\n",
            "\n",
            "üìà Orientation: long\n",
            "\n",
            "--- Sample Data ---\n",
            "   OrderID     Customer   OrderDate  Amount     Status DeliveryDate\n",
            "0     1001     John Doe  2023-01-01  150.75  Completed   2023-01-05\n",
            "1     1002   Jane Smith  2023-01-02    <NA>    Pending   2023-01-06\n",
            "2     1003  Peter Jones  2023-03-03   200.0  Completed   2023-03-07\n",
            "3     1004  Alice Brown  2023-04-04    75.2  Cancelled          NaN\n",
            "4     1005    Bob White  2023-05-05   120.0  Completed   2023-05-09\n",
            "\n",
            "--- Data Types ---\n",
            "  OrderID: int64 ‚Üí Int64\n",
            "  Customer: object ‚Üí string\n",
            "  OrderDate: object ‚Üí string\n",
            "  Amount: object ‚Üí Float64\n",
            "  Status: object ‚Üí category\n",
            "  DeliveryDate: object ‚Üí string\n",
            "‚ö†Ô∏è  Warnings (3):\n",
            "   ‚Ä¢ Column 'OrderDate' was coerced to datetime, but 12.50% of values were unparseable. Standardized to YYYY-MM-DD format.\n",
            "   ‚Ä¢ Column 'Amount' was coerced to numeric, but 12.50% of values were unparseable.\n",
            "   ‚Ä¢ Column 'DeliveryDate' was coerced to datetime, but 12.50% of values were unparseable. Standardized to YYYY-MM-DD format.\n"
          ]
        }
      ],
      "source": [
        "# Scenario 4: CSV with mixed types, varying date formats, and partial corruption\n",
        "\n",
        "df_processed_mixed, metadata_mixed = process_gym_data(csv_path_mixed_corrupt)\n",
        "\n",
        "print(\"\\n=== SCENARIO 4: Mixed Types & Corruption ===\")\n",
        "print(f\"\\nüìä DataFrame Shape: {df_processed_mixed.shape[0]} rows √ó {df_processed_mixed.shape[1]} columns\")\n",
        "print(f\"\\nüè∑Ô∏è  Columns Identified: {list(df_processed_mixed.columns)}\")\n",
        "print(f\"\\nüéØ Column Roles: {metadata_mixed.column_roles}\")\n",
        "print(f\"\\nüìÅ File Type: {metadata_mixed.file_type}\")\n",
        "print(f\"\\nüìà Orientation: {metadata_mixed.orientation}\")\n",
        "\n",
        "print(\"\\n--- Sample Data ---\")\n",
        "print(df_processed_mixed.head())\n",
        "\n",
        "print(\"\\n--- Data Types ---\")\n",
        "for col, dtype in metadata_mixed.inferred_dtypes.items():\n",
        "    original = metadata_mixed.original_dtypes.get(col, 'unknown')\n",
        "    print(f\"  {col}: {original} ‚Üí {dtype}\")\n",
        "\n",
        "if metadata_mixed.warnings:\n",
        "    print(f\"‚ö†Ô∏è  Warnings ({len(metadata_mixed.warnings)}):\")\n",
        "    for warning in metadata_mixed.warnings:\n",
        "        print(f\"   ‚Ä¢ {warning}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No warnings generated\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
