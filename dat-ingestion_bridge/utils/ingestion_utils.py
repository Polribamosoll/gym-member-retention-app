import pandas as pd\nimport os\nfrom io import StringIO, BytesIO\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Tuple, Union\n\n@dataclass\nclass IngestionMetadata:\n    file_type: str = \"\"\n    orientation: str = \"\"\n    column_roles: Dict[str, str] = field(default_factory=dict)\n    original_dtypes: Dict[str, str] = field(default_factory=dict)\n    inferred_dtypes: Dict[str, str] = field(default_factory=dict)\n    warnings: List[str] = field(default_factory=list)\n\ndef load_data(file_path_or_object: Union[str, StringIO, BytesIO]) -> Tuple[pd.DataFrame, str, List[str]]:\n    \"\"\"\n    Accepts a file path or file-like object, detects file type by extension,\n    loads into a pandas DataFrame, and handles common issues.\n\n    Args:\n        file_path_or_object: Path to the file (str) or a file-like object.\n\n    Returns:\n        Tuple[pd.DataFrame, str, List[str]]: A tuple containing:\n            - pd.DataFrame: Loaded DataFrame.\n            - str: Detected file type (e.g., \'csv\', \'xlsx\').\n            - List[str]: List of warnings encountered during loading.\n    \"\"\"\
    warnings: List[str] = []\n    file_type: str = \"unknown\"\n\n    if isinstance(file_path_or_object, str):\n        file_extension = os.path.splitext(file_path_or_object)[1].lower()\n        file_obj = file_path_or_object\n    else:\n        if hasattr(file_path_or_object, \'name\'):\n            file_extension = os.path.splitext(file_path_or_object.name)[1].lower()\n        else:\n            file_extension = \'.csv\'\n        file_obj = file_path_or_object\n\n    df = None\n    \n    encodings = [\'utf-8\', \'latin1\', \'iso-8859-1\']\n\n    if file_extension in [\'.csv\', \'.tsv\', \'.txt\']:\n        file_type = file_extension[1:]\n        delimiters = [\',\', \'\\\\t\', \';\', \'|\']\n        for encoding in encodings:\n            for delimiter in delimiters:\n                try:\n                    if isinstance(file_obj, str):\n                        df = pd.read_csv(file_obj, delimiter=delimiter, encoding=encoding, engine=\'python\')\n                    else:\n                        file_obj.seek(0)\n                        df = pd.read_csv(file_obj, delimiter=delimiter, encoding=encoding, engine=\'python\')\n                    \n                    if df.empty or df.shape[1] == 1: \n                        if df.shape[1] == 1: # If only one column, delimiter was likely wrong\n                            warnings.append(f\"Attempted to load with delimiter \'{delimiter}\' and encoding \'{encoding}\', but resulted in a single column. Trying other options.\")\n                        continue # Try next delimiter/encoding\n                    break # Successfully loaded\n                except Exception as e:\n                    warnings.append(f\"Failed to load with delimiter \'{delimiter}\' and encoding \'{encoding}\': {e}\")\n            if df is not None: # If loaded with any delimiter for this encoding, break encoding loop\n                break\n\n    elif file_extension in [\'.xlsx\', \'.xls\']:\n        file_type = file_extension[1:]\n        for encoding in encodings: # Encodings are less relevant for Excel but can sometimes be a factor for text within cells\n            try:\n                if isinstance(file_obj, str):\n                    df = pd.read_excel(file_obj, engine=\'openpyxl\' if file_extension == \'.xlsx\' else \'xlrd\')\n                else:\n                    file_obj.seek(0)\n                    df = pd.read_excel(file_obj, engine=\'openpyxl\' if file_extension == \'.xlsx\' else \'xlrd\')\n                break # Successfully loaded\n            except Exception as e:\n                warnings.append(f\"Failed to load Excel file with encoding \'{encoding}\': {e}\")\n    else:\n        warnings.append(f\"Unsupported file type detected: {file_extension}. Attempting to load as CSV.\")\n        file_type = \"csv_fallback\"\n        # Fallback to CSV loading if extension is unknown\n        delimiters = [\',\', \'\\\\t\', \';\', \'|\']\n        for encoding in encodings:\n            for delimiter in delimiters:\n                try:\n                    if isinstance(file_obj, str):\n                        df = pd.read_csv(file_obj, delimiter=delimiter, encoding=encoding, engine=\'python\')\n                    else:\n                        file_obj.seek(0)\n                        df = pd.read_csv(file_obj, delimiter=delimiter, encoding=encoding, engine=\'python\')\n                    if df.empty or df.shape[1] == 1: \n                        continue\n                    break\n                except Exception as e:\n                    warnings.append(f\"Fallback CSV loading failed with delimiter \'{delimiter}\' and encoding \'{encoding}\': {e}\")\n            if df is not None:\n                break\n\n    if df is None:\n        raise ValueError(\"Could not load the file with common encodings or delimiters after multiple attempts.\")\n\n    # Drop empty rows and columns - handles \"junk columns from exports\"\n    initial_rows, initial_cols = df.shape\n    df.dropna(axis=0, how=\'all\', inplace=True)\n    df.dropna(axis=1, how=\'all\', inplace=True)\n    if df.shape[0] < initial_rows:\n        warnings.append(f\"Dropped {initial_rows - df.shape[0]} entirely empty rows.\")\n    if df.shape[1] < initial_cols:\n        warnings.append(f\"Dropped {initial_cols - df.shape[1]} entirely empty columns.\")\n\n    return df, file_type, warnings\n\ndef infer_and_coerce_datatypes(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict[str, str], Dict[str, str], List[str]]:\n    \"\"\"\n    Infers and coerces data types from values, handling missing values,\n    mixed types, and multiple date formats.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        Tuple[pd.DataFrame, Dict[str, str], Dict[str, str], List[str]]: A tuple containing:\n            - pd.DataFrame: DataFrame with coerced data types.\n            - Dict[str, str]: Original dtypes before coercion.\n            - Dict[str, str]: Inferred dtypes after coercion.\n            - List[str]: List of warnings encountered during type inference.\n    \"\"\"\n    df_coerced = df.copy()\n    original_dtypes = {col: str(df[col].dtype) for col in df.columns}\n    inferred_dtypes: Dict[str, str] = {}\n    warnings: List[str] = []\n\n    for col in df_coerced.columns:\n        # Try to convert to numeric (Int64 or Float64)\n        numeric_series = pd.to_numeric(df_coerced[col], errors=\'coerce\')\n        numeric_ratio = numeric_series.notna().sum() / len(df_coerced)\n        \n        if numeric_ratio > 0.8: # Heuristic: 80% non-null numeric values\n            if (numeric_series == numeric_series.astype(pd.Int64Dtype())).all():\n                df_coerced[col] = numeric_series.astype(pd.Int64Dtype())\n                inferred_dtypes[col] = \'Int64\'\n            else:\n                df_coerced[col] = numeric_series.astype(pd.Float64Dtype())\n                inferred_dtypes[col] = \'Float64\'\n            if numeric_ratio < 1.0:\n                warnings.append(f\"Column \'{col}\' was coerced to numeric, but {100 * (1 - numeric_ratio):.2f}% of values were unparseable.\")\n            continue\n\n        # Try to convert to datetime\n        datetime_series = pd.to_datetime(df_coerced[col], errors=\'coerce\', dayfirst=True)\n        datetime_ratio = datetime_series.notna().sum() / len(df_coerced)\n\n        if datetime_ratio > 0.8: # Heuristic: 80% non-null datetime values\n            df_coerced[col] = datetime_series\n            inferred_dtypes[col] = \'datetime64[ns]\'\n            if datetime_ratio < 1.0:\n                warnings.append(f\"Column \'{col}\' was coerced to datetime, but {100 * (1 - datetime_ratio):.2f}% of values were unparseable or in inconsistent formats.\")\n            continue\n\n        # Try to convert to boolean\n        bool_temp_series = df_coerced[col].astype(str).str.lower().str.strip()\n        true_values = [\'true\', \'1\', \'yes\', \'y\']\n        false_values = [\'false\', \'0\', \'no\', \'n\']\n        bool_coerced_series = bool_temp_series.map(lambda x: True if x in true_values else (False if x in false_values else np.nan))\n        boolean_ratio = bool_coerced_series.notna().sum() / len(df_coerced)\n\n        if boolean_ratio > 0.8:\n            df_coerced[col] = bool_coerced_series.astype(pd.BooleanDtype())\n            inferred_dtypes[col] = \'boolean\'\n            if boolean_ratio < 1.0:\n                warnings.append(f\"Column \'{col}\' was coerced to boolean, but {100 * (1 - boolean_ratio):.2f}% of values were unparseable.\")\n            continue\n\n        # Fallback to string or category\n        # If unique values are less than 50% of total rows and more than 1, consider it categorical\n        if df_coerced[col].nunique() / len(df_coerced) < 0.5 and df_coerced[col].nunique() > 1:\n            df_coerced[col] = df_coerced[col].astype(\'category\')\n            inferred_dtypes[col] = \'category\'\n        else:\n            df_coerced[col] = df_coerced[col].astype(pd.StringDtype())\n            inferred_dtypes[col] = \'string\'\n            \n    return df_coerced, original_dtypes, inferred_dtypes, warnings\n\ndef infer_column_roles(df: pd.DataFrame) -> Dict[str, str]:\n    \"\"\"\n    Automatically classifies each column as Date/time, Numeric metric,\n    Categorical dimension, or Identifier.\n\n    Args:\n        df (pd.DataFrame): The input DataFrame (preferably after type coercion).\n\n    Returns:\n        Dict[str, str]: A dictionary where keys are column names and values are their inferred roles.\n    \"\"\"\
    column_roles: Dict[str, str] = {}\n    \n    for col in df.columns:\n        if pd.api.types.is_datetime64_any_dtype(df[col]):\n            column_roles[col] = \'Date/time\'\n        elif pd.api.types.is_numeric_dtype(df[col]):\n            if df[col].nunique() / len(df[col]) > 0.9 and len(df[col]) > 10: # Heuristic for identifier: >90% unique and more than 10 rows\n                column_roles[col] = \'Identifier\'\n            else:\n                column_roles[col] = \'Numeric metric\'\n        elif pd.api.types.is_categorical_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):\n            if df[col].nunique() / len(df[col]) > 0.9 and len(df[col]) > 10: # Heuristic for identifier: >90% unique and more than 10 rows\n                column_roles[col] = \'Identifier\'\n            else:\n                column_roles[col] = \'Categorical dimension\'\n        else:\n            column_roles[col] = \'Other\'\n\n    return column_roles\n\ndef detect_orientation(df: pd.DataFrame) -> str:\n    \"\"\"\n    Infers whether the data is \'wide\' (metrics as columns) or \'long\' (variable/value structure).\n\n    Args:\n        df (pd.DataFrame): The input DataFrame.\n\n    Returns:\n        str: \'wide\' or \'long\'.\n    \"\"\"\
    numeric_cols = df.select_dtypes(include=[\'number\']).columns\n    non_numeric_cols = df.select_dtypes(exclude=[\'number\']).columns\n\n    potential_value_cols = [\'value\', \'metric\', \'data\', \'count\']\n    potential_variable_cols = [\'variable\', \'category\', \'type\', \'attribute\']\n\n    has_value_col = any(col.lower() in potential_value_cols for col in non_numeric_cols)\n    has_variable_col = any(col.lower() in potential_variable_cols for col in non_numeric_cols)\n\n    if has_value_col and has_variable_col and len(numeric_cols) <=2 :\n        return \'long\'\n    \n    if len(numeric_cols) / df.shape[1] > 0.7:\n        return \'wide\'\n    elif len(non_numeric_cols) / df.shape[1] > 0.5 and len(numeric_cols) <= 2:\n        return \'long\'\n    \n    return \'wide\'\n    \ndef process_gym_data(file_path_or_object: Union[str, StringIO, BytesIO]) -> Tuple[pd.DataFrame, IngestionMetadata]:\n    \"\"\"\n    Orchestrates the data ingestion, type inference, and column role classification.\n\n    Args:\n        file_path_or_object: Path to the file (str) or a file-like object.\n\n    Returns:\n        Tuple[pd.DataFrame, IngestionMetadata]: A tuple containing:\n            - pd.DataFrame: The processed DataFrame with inferred types.\n            - IngestionMetadata: A dataclass object containing all inferred metadata and warnings.\n    \"\"\"\
    metadata = IngestionMetadata()\n\n    df_raw, file_type, load_warnings = load_data(file_path_or_object)\n    metadata.file_type = file_type\n    metadata.warnings.extend(load_warnings)\n    \n    df_processed, original_dtypes, inferred_dtypes, type_warnings = infer_and_coerce_datatypes(df_raw)\n    metadata.original_dtypes = original_dtypes\n    metadata.inferred_dtypes = inferred_dtypes\n    metadata.warnings.extend(type_warnings)\n\n    metadata.column_roles = infer_column_roles(df_processed)\n    metadata.orientation = detect_orientation(df_processed)\n\n    return df_processed, metadata\n